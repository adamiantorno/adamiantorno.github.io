[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.17.1","content-config-digest","a0f5e01e0107f46a","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","projects",["Map",11,12,53,54,94,95,153,154,181,182,243,244,278,279],"arduino-enclosure",{"id":11,"data":13,"body":27,"filePath":28,"assetImports":29,"digest":34,"rendered":35},{"project_id":14,"title":15,"description":16,"categories":17,"tags":19,"image":25,"date_string":26},4,"Arduino Enclosure","A custom enclosure with LCD intergration and hardware UI to control a test fixture.",[18],"Mechanical",[20,21,22,23,24],"SolidWorks","I2C","3D Printing","Soldering","Sourcing Components","src/data/images/coverArduino.jpg","Winter 2022","# Overview\r\n\r\nThis was a subproject for the Hardware Testing fixture project. After the entire HIL test fixture had been assembled and was operating as expected, there were three main points of improvement. First, the switches were too small and soldered too close together which made it difficult to operate one without affecting another. Second, the LCD was also tilted at an angle that made it difficult to see when standing above the test fixture. Finally, all wiring should be hidden or strapped down to prevent it from interacting with or getting confused for the test fixture’s wire harness.\r\n\r\n![Initial implementation of test fixture arduino](../images/arduinoInitial.png)\r\n*Initial implementation of test fixture arduino*\r\n\r\n---\r\n\r\n# Solution\r\n\r\nAs a solution, I designed an enclosure that could contain the arduino circuit, support the LCD at a softer angle, and have larger switches in a more ergonomic position. The enclosure was separated into a bottom half and top half. The bottom half of the enclosure had mounts for the protoboard and CAN module, a hole on the rear side for output wires, and a lot of extra space to keep all internal wires contained. To maximize user experience, the new, larger switches were sourced with the final approval from the engineers at Electrans since they would be operating the fixture. To connected to bottom and top halves of the enclosure, I designed a custom latching system so that the engineers could easily reomve the top half without having to unmount the entire enclosure. Additionally, the enclosure was mounted using the same screws and design as the aforementioned mounting brackets and screws for the mechanical components.\r\n\r\n![3D CAD of Arduino enclosure](../images/arduinoCad.png)\r\n![Arduino inside mounted enclosure](../images/arduinoSolder.jpeg)\r\n![Final version of the product](../images/coverArduino.jpg)","src/data/projects/arduino-enclosure.md",[30,31,32,33],"../images/arduinoInitial.png","../images/arduinoCad.png","../images/arduinoSolder.jpeg","../images/coverArduino.jpg","cf5180c068d3eca0",{"html":36,"metadata":37},"\u003Ch1 id=\"overview\">Overview\u003C/h1>\n\u003Cp>This was a subproject for the Hardware Testing fixture project. After the entire HIL test fixture had been assembled and was operating as expected, there were three main points of improvement. First, the switches were too small and soldered too close together which made it difficult to operate one without affecting another. Second, the LCD was also tilted at an angle that made it difficult to see when standing above the test fixture. Finally, all wiring should be hidden or strapped down to prevent it from interacting with or getting confused for the test fixture’s wire harness.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/arduinoInitial.png&#x22;,&#x22;alt&#x22;:&#x22;Initial implementation of test fixture arduino&#x22;,&#x22;index&#x22;:0}\">\r\n\u003Cem>Initial implementation of test fixture arduino\u003C/em>\u003C/p>\n\u003Chr>\n\u003Ch1 id=\"solution\">Solution\u003C/h1>\n\u003Cp>As a solution, I designed an enclosure that could contain the arduino circuit, support the LCD at a softer angle, and have larger switches in a more ergonomic position. The enclosure was separated into a bottom half and top half. The bottom half of the enclosure had mounts for the protoboard and CAN module, a hole on the rear side for output wires, and a lot of extra space to keep all internal wires contained. To maximize user experience, the new, larger switches were sourced with the final approval from the engineers at Electrans since they would be operating the fixture. To connected to bottom and top halves of the enclosure, I designed a custom latching system so that the engineers could easily reomve the top half without having to unmount the entire enclosure. Additionally, the enclosure was mounted using the same screws and design as the aforementioned mounting brackets and screws for the mechanical components.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/arduinoCad.png&#x22;,&#x22;alt&#x22;:&#x22;3D CAD of Arduino enclosure&#x22;,&#x22;index&#x22;:0}\">\r\n\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/arduinoSolder.jpeg&#x22;,&#x22;alt&#x22;:&#x22;Arduino inside mounted enclosure&#x22;,&#x22;index&#x22;:0}\">\r\n\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/coverArduino.jpg&#x22;,&#x22;alt&#x22;:&#x22;Final version of the product&#x22;,&#x22;index&#x22;:0}\">\u003C/p>",{"headings":38,"localImagePaths":46,"remoteImagePaths":47,"frontmatter":48,"imagePaths":52},[39,43],{"depth":40,"slug":41,"text":42},1,"overview","Overview",{"depth":40,"slug":44,"text":45},"solution","Solution",[30,31,32,33],[],{"layout":49,"project_id":14,"title":15,"description":16,"categories":50,"tags":51,"image":25,"date_string":26},"../../layouts/markdown.astro",[18],[20,21,22,23,24],[30,31,32,33],"capstone",{"id":53,"data":55,"body":69,"filePath":70,"assetImports":71,"digest":73,"rendered":74},{"project_id":56,"title":57,"description":58,"categories":59,"tags":62,"image":67,"date_string":68},7,"Automated Pressure Relief Mattress","A 16-cell air mattress with automated inflation to prevent pressure ulcers, featuring a mobile app for real-time monitoring and manual control",[60,61],"Electrical","Software",[63,64,21,65,66],"Python","Raspberry Pi","Mutlithreading","PCB Design","src/data/images/coverTurnCare.JPEG","Sept 2023 - Mar 2024","# Overview\r\nTODO\r\n\r\n# Features\r\nTODO\r\n\r\n![Group symposium photo](../images/coverTurnCare.JPEG)\r\n\r\n# Learnings\r\nTODO\r\n\r\n# Challenges\r\nTODO","src/data/projects/capstone.md",[72],"../images/coverTurnCare.JPEG","f0c8dff1b0fd780e",{"html":75,"metadata":76},"\u003Ch1 id=\"overview\">Overview\u003C/h1>\n\u003Cp>TODO\u003C/p>\n\u003Ch1 id=\"features\">Features\u003C/h1>\n\u003Cp>TODO\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/coverTurnCare.JPEG&#x22;,&#x22;alt&#x22;:&#x22;Group symposium photo&#x22;,&#x22;index&#x22;:0}\">\u003C/p>\n\u003Ch1 id=\"learnings\">Learnings\u003C/h1>\n\u003Cp>TODO\u003C/p>\n\u003Ch1 id=\"challenges\">Challenges\u003C/h1>\n\u003Cp>TODO\u003C/p>",{"headings":77,"localImagePaths":88,"remoteImagePaths":89,"frontmatter":90,"imagePaths":93},[78,79,82,85],{"depth":40,"slug":41,"text":42},{"depth":40,"slug":80,"text":81},"features","Features",{"depth":40,"slug":83,"text":84},"learnings","Learnings",{"depth":40,"slug":86,"text":87},"challenges","Challenges",[72],[],{"project_id":56,"title":57,"description":58,"categories":91,"tags":92,"image":67,"date_string":68},[60,61],[63,64,21,65,66],[72],"hil",{"id":94,"data":96,"body":107,"filePath":108,"assetImports":109,"digest":115,"rendered":116},{"project_id":97,"title":98,"description":99,"categories":100,"tags":101,"image":106,"date_string":26},5,"Hardware-in-Loop Test Fixture","A pneuamtic and electrical fixture to verify PCB design and firmware logic before integrating them into a production environment",[18,60,61],[102,103,20,104,105],"C++","Arduino","Altium365","CAN (J1939)","src/data/images/coverHil.jpg","# Objective\r\n\r\nThe objective of this project was to design and build a Hardware Testing Fixture for the Printed Circuit Boards (PCBs) for a new Electronic Control Unit (ECU) that would be integrated into a heavy-duty vehicle's CAN bus. The exact details of the ECU are confidential to Electrans Technologies and therefore are not included in this report. The test fixture had the same mechanical actuating components and sensors as the final system to emulate their integration with the controller in an isolated environment.\r\n\r\n# Background Information\r\n\r\nThe Controller Area Network (CAN) is a two-wire vehicle bus that is an international standard for the automotive industry known as ISO 11898. Compared to previous dedicated wire harnesses used in vehicles, the CAN bus is a high-integrity system that reduces wiring cost, complexity, and weight. Components connected through the CAN bus are referred to as nodes or electronic control units (ECUs). ECUs communicate by broadcasting information, such as sensor data, onto the CAN bus and all others ECU can check the data and decide whether to receive or ignore it”. Data broadcasted across the CAN bus are sent in packets called frames. A CAN frame consists of 8 parts, which in the adjacent figure.\r\n\r\nDifferent vehicles and automotive companies may use different CAN communication protocols in their systems. This is because, while standard CAN is extremely effective in automotive and smaller embedded applications, it is not alone suitable for networks and messages with more than 8 bytes. The Society of Automotive Engineers (SAE) developed SAE J1939 which is a higher-layer protocol based on standard CAN communication. J1939 is an industry standard heavy-duty vehicles and off-highway machines such as construction, material handling, mass transportation, forestry machines, agricultural machinery, maritime and military applications. There are several key characteristics of the J1939 protocol. Most notably, J1939 extends the CAN identifier from 11-bits to 29-bits by adding an 18-bit unique frame identifier known as the Parameter Group Number (PGN). As an example, based on the SAE J1939-7 documentation, a J1939 message with a PGN of 61444 (HEX ID 0F004), represents the “Electronic Engine Controller 1 – EEC1”. Additionally, its signals sent in the data bytes are identified with Suspect Parameter Numbers (SPN). They can be described in terms of their bit start position, bit length, scale, offset and unit.\r\n\r\n# Problem Definition\r\n\r\nElectrans Technologies is in the process of the designing a new ECU for heavy-duty vehicles. Electrans has completed the 3D CAD model, design the central PCB, and programming its firmware. As Electrans transitions to full assembly of the ECU, it is imperative that the start-up properly tests the controller’s compatibility with the ECU’s hardware - sensors, pneumatics, motors. If not, Electrans risks the chance of damaging the assembly or missing key points of improvement. Additionally, by not testing their controller in parallel with the assembly of the ECU, Electrans will create a bottleneck in their design process. This is because certain changes to the controller could be realized now through testing while the ECU is being assembled rather waiting for assembly to finish and tsetings afterwards. Therefore, Electrans requires a test fixture to test the main controller with the mechanical components of the system before adding it to the final assembly.\r\n\r\n## Requirements \r\n\r\n- The fixture shall have the same set of actuators and sensors as the new ECU to emulate their inputs and outputs with the controller board in the main assembly\r\n- The fixture shall use an Arduino and physical switches to create CAN signals to mimic CAN communication with a heavy-duty vehicle.\r\n- The fixture shall hold removable metal pieces to activate any pairs of proximity sensors together.\r\n- The controller device(s) under test can be placed into or removed from the fixture any modifications to the hardware\r\n- The fixture’s hardware shall have hard emergency e-stop\r\n\r\n## Constraints\r\n\r\nFirst, the test fixture must be easy to operate. Engineers will likely be making continuous changes to the controller during the design process and should be able to make those changes very easily.\r\n\r\nSecondly, the design process of the test fixture must minimize costs. This will increase the profitability of the overall ECU project.\r\n\r\n# Design\r\n\r\nBased on HILSR-1, the Hardware-In-Loop (HIL) test fixture must have the same mechanical components as the final system. However, in the final system, there are two pairs of cylinders which are connected to the same air flow. Therefore, the test fixture will require two less cylinders than the ECU since only one cylinder is required per pair to demonstrate their behavior. Consequently, there will also be one less auto switch on the test fixture since it would be mounted on one of the paired cylinders in the final system.\r\n\r\nThe purpose of the test fixture is to analyze the operation controller with mechanical components in an isolated environment. Therefore, all of the components were mounted onto a horizontal, wooden board. This made it easy to see all of the components at once and mount them quickly using screws. This exlcudes the auoswtiches which were mounted to the pneuamtic cylinders to indicate the state of the cyclinder with an LED.\r\n\r\nTo secure the components in an optimal orientation – for instance the cylinders oriented horizontally and the stepper motor upwards – mounting brackets were designed and 3D printed for nearly all of the components. \r\n\r\n![3D Printed mounts for pneumatic cylinders, proximity sensors, latch, and motor](../images/hilMounts.png)\r\n\r\nThe only requirement for the location of the mechanical components on the test fixture was that the inductive proximity sensors must react to the cylinders and trigger a signal when they are fully extended. Therefore, the proximity sensors were placed opposite to the cylinders, 8mm from the fully extended position of the rod. This was based on the proximity sensors’ range specification so the sensor would only trigger when the cylinder was fully extended. The placement of the rest of the components was based on weight distribution and keeping the controller and Arduino along of the edges of the fixture. This would make it easier for the engineers to reach the two components.\r\n\r\n![Full CAD of HIL Fixture](../images/hilCad.png)\r\n\r\n# Electrical Design\r\n\r\nAn Arduino Nano was used as the main board for the selected swtiches. The test fixture’s Arduino program was separated into three main tasks: initializing and updating the switches, updating the LCD display, and sending CAN messages. Each switch had its own class, written in its own separate .cpp and header file.. Each switch class had both private and public members. The private members included the current state of the switch, the switch’s input pin on the Arduino, and, for the potentiometer, the interval of values to select from. The public members included the switch’s own initialization and update functions, which would be called from the main .ino file.\r\n\r\nThe LCD display was connected to the Arduino using Inter-Integrated Circuit (I2C) protocol. I2C is a short distance communication bus that only requires two signal wires to exchange information: Serial Data (SDA) and Serial Clock (SCL). This is much more efficient than connecting directly to the LCD display since that would require twelve signal wires. Based on the Arduino pinout, the SDA and SCL wires of the LCD were connected to pin 23 and 24 of the Arduino, respectively. In the firmware, the LCD was controlled using the Arduino library “LiquidCrystal_I2C” which was written by Frank de Brabander and is maintained by Marco Schwartz. In each of the switches update function, the LCD was updated to display the current state of the switch being updated. \r\n\r\n![HIL Electrical Schematic](../images/hilArduinoCircuit.png)\r\n![HIL Electrical Circuit Breadboard](../images/hilArduinoBread.jpg)\r\n\r\n# Software \r\n\r\nThere are four signals that the ECU will receive from the vehicle's CAN Bus: engine speed, current gear, start signal, and latch state. . For each signal, an appropriate switch was selected for the HIL Fixture. For the engine speed signal, a linear potentiometer was selected since it allows users to choose any value within a set interval based on the resistance of the switch. For the current gear signal, a 3-position rotary encoder was selected to choose between a parked, reverse, and forward state. For the start signal, a mom-off (momentarily on/off) push button was selected since the start signal only needed to be sent for a moment. Finally, for the state of the latch signal, a on-off toggle switch was selected since the latch only has two states: connected and disconnected. While the start signal and the latch state signal are specific to the ECU, the engine speed and current gear signals can be found in the SAE J1939-7 documentation.\r\n\r\nTo create and send CAN signals from the Arduino, a Serial CAN bus module was connected to the circuit. This module was able to turn the Arduino’s UART signals into CAN signals that were to be sent to the main controller board. Since the Arduino was focused on sending specific signals rather than an entire CAN frame, a custom function was created to properly place a switch’s CAN signal into a CAN frame without affecting the rest of the data of the frame:\r\n\r\nAs parameters, the function takes an uint8_t array with eight elements (each element represents a byte from the original CAN frame) that is passed by reference, the value of the signal, the start bit of the signal within the CAN frame, and the length of the signal. First, the array is read into a uint64_t variable with an 8-iteration for-loop that saves an element of the array into the uint64-t variable then shifts the bits of the variable to the left by 8 before repeating the process with the next element of the array. After the for-loop, a second uint64-t variable is created equal to the value of the signal that was passed as a parameter. This variable is then left shifted to the start position of the signal. Then, the original uint64_t variable is passed through an AND mask to clear the bits in the position signal, and then passed through an OR mask with the uint64_t variable equal to the value of the signal to successfully save the signal into the uint64_t variable without affecting any other bits. Finally, the uint64_t variable is saved into the array that was passed by reference with another 8-iteration for-loop and right shifting the bits.\r\n\r\nThe CAN signals from the Arduino circuit were monitored with a Vector CANalyzer tool to ensure that the Serial-to-CAN module and custom CAN message function was working as expected. Once all components, connections, and firmware had been tested on the breadboard, all components and wires were soldered to a protoboard and mounted to the test fixture.\r\n\r\n# Final Design\r\n\r\nTo connect all of these components to the controller, a wire harness needed to be designed and assembled. When examining each of the component's signal wires, the auto switches had the smallest wire gauge of 24 AWG, while the rest of the components had a 22 AWG wire gauge. Therefore, I selected the MCON 1.2 series connector for the wire harness since they are compatible with 24-16 AWG wire and extremely durable and safe with an IP of 6K9K. Additionally, the MCON connectors range from 2 to 6 position connectors, therefore a connector is available for all components except the pneumatic manifold. The pneumatic manifold uses a 25-pin D-Sub connector which was also sourced externally.\r\n\r\nThe completed HIL fixture is used only internally at Electrans for engineers to test their controllers for the current design of the final system Any new iterations to the controller or final ECU product will need to be reflected in the HIL fixture. This could be adding or removing a component or changing connectors to the controller. Therefore, the test fixture will likely require consistent updates to remain up to date and effectively test the controller before it is placed into the vehicle.\r\n\r\n![Final version of the product](../images/coverHil.jpg)","src/data/projects/hil.md",[110,111,112,113,114],"../images/hilMounts.png","../images/hilCad.png","../images/hilArduinoCircuit.png","../images/hilArduinoBread.jpg","../images/coverHil.jpg","3afdc457eaa48ec1",{"html":117,"metadata":118},"\u003Ch1 id=\"objective\">Objective\u003C/h1>\n\u003Cp>The objective of this project was to design and build a Hardware Testing Fixture for the Printed Circuit Boards (PCBs) for a new Electronic Control Unit (ECU) that would be integrated into a heavy-duty vehicle’s CAN bus. The exact details of the ECU are confidential to Electrans Technologies and therefore are not included in this report. The test fixture had the same mechanical actuating components and sensors as the final system to emulate their integration with the controller in an isolated environment.\u003C/p>\n\u003Ch1 id=\"background-information\">Background Information\u003C/h1>\n\u003Cp>The Controller Area Network (CAN) is a two-wire vehicle bus that is an international standard for the automotive industry known as ISO 11898. Compared to previous dedicated wire harnesses used in vehicles, the CAN bus is a high-integrity system that reduces wiring cost, complexity, and weight. Components connected through the CAN bus are referred to as nodes or electronic control units (ECUs). ECUs communicate by broadcasting information, such as sensor data, onto the CAN bus and all others ECU can check the data and decide whether to receive or ignore it”. Data broadcasted across the CAN bus are sent in packets called frames. A CAN frame consists of 8 parts, which in the adjacent figure.\u003C/p>\n\u003Cp>Different vehicles and automotive companies may use different CAN communication protocols in their systems. This is because, while standard CAN is extremely effective in automotive and smaller embedded applications, it is not alone suitable for networks and messages with more than 8 bytes. The Society of Automotive Engineers (SAE) developed SAE J1939 which is a higher-layer protocol based on standard CAN communication. J1939 is an industry standard heavy-duty vehicles and off-highway machines such as construction, material handling, mass transportation, forestry machines, agricultural machinery, maritime and military applications. There are several key characteristics of the J1939 protocol. Most notably, J1939 extends the CAN identifier from 11-bits to 29-bits by adding an 18-bit unique frame identifier known as the Parameter Group Number (PGN). As an example, based on the SAE J1939-7 documentation, a J1939 message with a PGN of 61444 (HEX ID 0F004), represents the “Electronic Engine Controller 1 – EEC1”. Additionally, its signals sent in the data bytes are identified with Suspect Parameter Numbers (SPN). They can be described in terms of their bit start position, bit length, scale, offset and unit.\u003C/p>\n\u003Ch1 id=\"problem-definition\">Problem Definition\u003C/h1>\n\u003Cp>Electrans Technologies is in the process of the designing a new ECU for heavy-duty vehicles. Electrans has completed the 3D CAD model, design the central PCB, and programming its firmware. As Electrans transitions to full assembly of the ECU, it is imperative that the start-up properly tests the controller’s compatibility with the ECU’s hardware - sensors, pneumatics, motors. If not, Electrans risks the chance of damaging the assembly or missing key points of improvement. Additionally, by not testing their controller in parallel with the assembly of the ECU, Electrans will create a bottleneck in their design process. This is because certain changes to the controller could be realized now through testing while the ECU is being assembled rather waiting for assembly to finish and tsetings afterwards. Therefore, Electrans requires a test fixture to test the main controller with the mechanical components of the system before adding it to the final assembly.\u003C/p>\n\u003Ch2 id=\"requirements\">Requirements\u003C/h2>\n\u003Cul>\n\u003Cli>The fixture shall have the same set of actuators and sensors as the new ECU to emulate their inputs and outputs with the controller board in the main assembly\u003C/li>\n\u003Cli>The fixture shall use an Arduino and physical switches to create CAN signals to mimic CAN communication with a heavy-duty vehicle.\u003C/li>\n\u003Cli>The fixture shall hold removable metal pieces to activate any pairs of proximity sensors together.\u003C/li>\n\u003Cli>The controller device(s) under test can be placed into or removed from the fixture any modifications to the hardware\u003C/li>\n\u003Cli>The fixture’s hardware shall have hard emergency e-stop\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"constraints\">Constraints\u003C/h2>\n\u003Cp>First, the test fixture must be easy to operate. Engineers will likely be making continuous changes to the controller during the design process and should be able to make those changes very easily.\u003C/p>\n\u003Cp>Secondly, the design process of the test fixture must minimize costs. This will increase the profitability of the overall ECU project.\u003C/p>\n\u003Ch1 id=\"design\">Design\u003C/h1>\n\u003Cp>Based on HILSR-1, the Hardware-In-Loop (HIL) test fixture must have the same mechanical components as the final system. However, in the final system, there are two pairs of cylinders which are connected to the same air flow. Therefore, the test fixture will require two less cylinders than the ECU since only one cylinder is required per pair to demonstrate their behavior. Consequently, there will also be one less auto switch on the test fixture since it would be mounted on one of the paired cylinders in the final system.\u003C/p>\n\u003Cp>The purpose of the test fixture is to analyze the operation controller with mechanical components in an isolated environment. Therefore, all of the components were mounted onto a horizontal, wooden board. This made it easy to see all of the components at once and mount them quickly using screws. This exlcudes the auoswtiches which were mounted to the pneuamtic cylinders to indicate the state of the cyclinder with an LED.\u003C/p>\n\u003Cp>To secure the components in an optimal orientation – for instance the cylinders oriented horizontally and the stepper motor upwards – mounting brackets were designed and 3D printed for nearly all of the components.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/hilMounts.png&#x22;,&#x22;alt&#x22;:&#x22;3D Printed mounts for pneumatic cylinders, proximity sensors, latch, and motor&#x22;,&#x22;index&#x22;:0}\">\u003C/p>\n\u003Cp>The only requirement for the location of the mechanical components on the test fixture was that the inductive proximity sensors must react to the cylinders and trigger a signal when they are fully extended. Therefore, the proximity sensors were placed opposite to the cylinders, 8mm from the fully extended position of the rod. This was based on the proximity sensors’ range specification so the sensor would only trigger when the cylinder was fully extended. The placement of the rest of the components was based on weight distribution and keeping the controller and Arduino along of the edges of the fixture. This would make it easier for the engineers to reach the two components.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/hilCad.png&#x22;,&#x22;alt&#x22;:&#x22;Full CAD of HIL Fixture&#x22;,&#x22;index&#x22;:0}\">\u003C/p>\n\u003Ch1 id=\"electrical-design\">Electrical Design\u003C/h1>\n\u003Cp>An Arduino Nano was used as the main board for the selected swtiches. The test fixture’s Arduino program was separated into three main tasks: initializing and updating the switches, updating the LCD display, and sending CAN messages. Each switch had its own class, written in its own separate .cpp and header file.. Each switch class had both private and public members. The private members included the current state of the switch, the switch’s input pin on the Arduino, and, for the potentiometer, the interval of values to select from. The public members included the switch’s own initialization and update functions, which would be called from the main .ino file.\u003C/p>\n\u003Cp>The LCD display was connected to the Arduino using Inter-Integrated Circuit (I2C) protocol. I2C is a short distance communication bus that only requires two signal wires to exchange information: Serial Data (SDA) and Serial Clock (SCL). This is much more efficient than connecting directly to the LCD display since that would require twelve signal wires. Based on the Arduino pinout, the SDA and SCL wires of the LCD were connected to pin 23 and 24 of the Arduino, respectively. In the firmware, the LCD was controlled using the Arduino library “LiquidCrystal_I2C” which was written by Frank de Brabander and is maintained by Marco Schwartz. In each of the switches update function, the LCD was updated to display the current state of the switch being updated.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/hilArduinoCircuit.png&#x22;,&#x22;alt&#x22;:&#x22;HIL Electrical Schematic&#x22;,&#x22;index&#x22;:0}\">\r\n\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/hilArduinoBread.jpg&#x22;,&#x22;alt&#x22;:&#x22;HIL Electrical Circuit Breadboard&#x22;,&#x22;index&#x22;:0}\">\u003C/p>\n\u003Ch1 id=\"software\">Software\u003C/h1>\n\u003Cp>There are four signals that the ECU will receive from the vehicle’s CAN Bus: engine speed, current gear, start signal, and latch state. . For each signal, an appropriate switch was selected for the HIL Fixture. For the engine speed signal, a linear potentiometer was selected since it allows users to choose any value within a set interval based on the resistance of the switch. For the current gear signal, a 3-position rotary encoder was selected to choose between a parked, reverse, and forward state. For the start signal, a mom-off (momentarily on/off) push button was selected since the start signal only needed to be sent for a moment. Finally, for the state of the latch signal, a on-off toggle switch was selected since the latch only has two states: connected and disconnected. While the start signal and the latch state signal are specific to the ECU, the engine speed and current gear signals can be found in the SAE J1939-7 documentation.\u003C/p>\n\u003Cp>To create and send CAN signals from the Arduino, a Serial CAN bus module was connected to the circuit. This module was able to turn the Arduino’s UART signals into CAN signals that were to be sent to the main controller board. Since the Arduino was focused on sending specific signals rather than an entire CAN frame, a custom function was created to properly place a switch’s CAN signal into a CAN frame without affecting the rest of the data of the frame:\u003C/p>\n\u003Cp>As parameters, the function takes an uint8_t array with eight elements (each element represents a byte from the original CAN frame) that is passed by reference, the value of the signal, the start bit of the signal within the CAN frame, and the length of the signal. First, the array is read into a uint64_t variable with an 8-iteration for-loop that saves an element of the array into the uint64-t variable then shifts the bits of the variable to the left by 8 before repeating the process with the next element of the array. After the for-loop, a second uint64-t variable is created equal to the value of the signal that was passed as a parameter. This variable is then left shifted to the start position of the signal. Then, the original uint64_t variable is passed through an AND mask to clear the bits in the position signal, and then passed through an OR mask with the uint64_t variable equal to the value of the signal to successfully save the signal into the uint64_t variable without affecting any other bits. Finally, the uint64_t variable is saved into the array that was passed by reference with another 8-iteration for-loop and right shifting the bits.\u003C/p>\n\u003Cp>The CAN signals from the Arduino circuit were monitored with a Vector CANalyzer tool to ensure that the Serial-to-CAN module and custom CAN message function was working as expected. Once all components, connections, and firmware had been tested on the breadboard, all components and wires were soldered to a protoboard and mounted to the test fixture.\u003C/p>\n\u003Ch1 id=\"final-design\">Final Design\u003C/h1>\n\u003Cp>To connect all of these components to the controller, a wire harness needed to be designed and assembled. When examining each of the component’s signal wires, the auto switches had the smallest wire gauge of 24 AWG, while the rest of the components had a 22 AWG wire gauge. Therefore, I selected the MCON 1.2 series connector for the wire harness since they are compatible with 24-16 AWG wire and extremely durable and safe with an IP of 6K9K. Additionally, the MCON connectors range from 2 to 6 position connectors, therefore a connector is available for all components except the pneumatic manifold. The pneumatic manifold uses a 25-pin D-Sub connector which was also sourced externally.\u003C/p>\n\u003Cp>The completed HIL fixture is used only internally at Electrans for engineers to test their controllers for the current design of the final system Any new iterations to the controller or final ECU product will need to be reflected in the HIL fixture. This could be adding or removing a component or changing connectors to the controller. Therefore, the test fixture will likely require consistent updates to remain up to date and effectively test the controller before it is placed into the vehicle.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/coverHil.jpg&#x22;,&#x22;alt&#x22;:&#x22;Final version of the product&#x22;,&#x22;index&#x22;:0}\">\u003C/p>",{"headings":119,"localImagePaths":147,"remoteImagePaths":148,"frontmatter":149,"imagePaths":152},[120,123,126,129,133,136,139,142,144],{"depth":40,"slug":121,"text":122},"objective","Objective",{"depth":40,"slug":124,"text":125},"background-information","Background Information",{"depth":40,"slug":127,"text":128},"problem-definition","Problem Definition",{"depth":130,"slug":131,"text":132},2,"requirements","Requirements",{"depth":130,"slug":134,"text":135},"constraints","Constraints",{"depth":40,"slug":137,"text":138},"design","Design",{"depth":40,"slug":140,"text":141},"electrical-design","Electrical Design",{"depth":40,"slug":143,"text":61},"software",{"depth":40,"slug":145,"text":146},"final-design","Final Design",[110,111,112,113,114],[],{"project_id":97,"title":98,"description":99,"categories":150,"tags":151,"image":106,"date_string":26},[18,60,61],[102,103,20,104,105],[110,111,112,113,114],"brackets",{"id":153,"data":155,"body":164,"filePath":165,"assetImports":166,"digest":169,"rendered":170},{"project_id":156,"title":157,"description":158,"categories":159,"tags":160,"image":163,"date_string":26},3,"Sheet Metal Brackets","Design and drawings of mounting brackets for pneuamtincs cylinders on truck.",[18],[20,161,162],"Sheet Metal","Engineering Drawings","src/data/images/coverBracket.jpg","# Overview\r\n\r\nFor this project I modelled two differnet metal brackets in Solidworks that would support large pneumatic cylinders for a prototype. I then created engineering drawings of both brackets in solidworks to be sent to an overseas manufacturer.\r\n\r\n![Bracket drawings & built](../images/bracketDisplay.png)\r\n\r\n![Bracket Installed](../images/coverBracket.jpg)","src/data/projects/brackets.md",[167,168],"../images/bracketDisplay.png","../images/coverBracket.jpg","9a4ae1bce965ec25",{"html":171,"metadata":172},"\u003Ch1 id=\"overview\">Overview\u003C/h1>\n\u003Cp>For this project I modelled two differnet metal brackets in Solidworks that would support large pneumatic cylinders for a prototype. I then created engineering drawings of both brackets in solidworks to be sent to an overseas manufacturer.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/bracketDisplay.png&#x22;,&#x22;alt&#x22;:&#x22;Bracket drawings &#x26; built&#x22;,&#x22;index&#x22;:0}\">\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/coverBracket.jpg&#x22;,&#x22;alt&#x22;:&#x22;Bracket Installed&#x22;,&#x22;index&#x22;:0}\">\u003C/p>",{"headings":173,"localImagePaths":175,"remoteImagePaths":176,"frontmatter":177,"imagePaths":180},[174],{"depth":40,"slug":41,"text":42},[167,168],[],{"project_id":156,"title":157,"description":158,"categories":178,"tags":179,"image":163,"date_string":26},[18],[20,161,162],[167,168],"pick-and-place",{"id":181,"data":183,"body":192,"filePath":193,"assetImports":194,"digest":203,"rendered":204},{"project_id":184,"title":185,"description":186,"categories":187,"tags":188,"image":190,"date_string":191},6,"Autonomous Robot Arm","A pick-and-place manipulator that uses object detection and inverse kinematics to move objects autonomously",[61,60],[63,102,103,189],"Motor Drivers","src/data/images/coverRobotArm.png","Jan 2023 - May 2023","# Overview\r\n\r\nMy favourite course during my 3rd year at the University of Waterloo was definitely ME380 – Mechanical Engineering Design Workshop. The course was based on a term project where each group of students had to design and build a pick-and-place manipulator following specific constraints and requirements. I really enjoyed how this course pushed groups through each phase of the engineering design process and included the construction of a working prototype. It was also really interesting to see the different groups’ final manipulators and showcasing how group successfully tackled the constraints and requirements in a different manner.\r\n\r\nOur team chose to design a manipulator that was also autonomous and capable of dealing objects with complex geometries. This meant the manipulator needed to detect objects, calculate required movements, and handle objects ofvarious shapes and sizes, specifically those that are difficult to grasp. For the project, I was mainly focused on the electric and firmware aspects of the manipulator, including computer vision and motor control. However, everyone on the team wore many hats and was involved in all major design decisions.\r\n\r\n## Learnings\r\n\r\n# Needs Analysis\r\n\r\nA need exists for a manipulator to lift and move a complex geometry object from a specified pick-up location to a specified drop-off zone autonomously without user intervention. The system must also feature the capability to handle adversarial objects. \r\n\r\n## Design Specification\r\n\r\nThe design constraints and criteria related to the course requirements and our team’s own desire for autonomous and adversarial grasps is shown in the table below.\r\n\r\n![Robot Arm Engineering Spec](../images/robotArmEngSpec.png)\r\n\r\n# Design Process\r\n\r\n## Gripper Design\r\n\r\nWe explored three initial concepts for the gripper, with the focus on being able to pick up challenging and adversarial objects. Each of the designed grippers were to be powered by a micro-servo motor which was mounted into a rack-and-pinion system to translate a cylindrical rod that would open and close the gripper’s prongs through attached levers. All components of the gripper were 3D-printed except for the rack which was aluminum-based, the servo motor, and the bolts to mount the servo motor, this was to reduce the final moment arm of the gripper on the base of the manipulator for easier operation.\r\n\r\nThe final design was a three-pronged gripper with an angled fin edge. This was chosen for its adaptability to various object geometries, as the fins would increase the contact area with the object and ensure grip by scooping the objects. The fins also acted as a cage which prevented the object from falling out of grip during operation of the manipulator – a common issue with the other designs.\r\n\r\n![Robot Arm Gripper Designs](../images/robotArmGripperDesigns.png)\r\n\r\n## Manipulator Design\r\n\r\nSimilarly, three designs were considered for the manipulator. This included a 4-DOF articulated arm, a crane-pulley system, and a 3D prismatic manipulator. Ultimately, the 4-DOF arm was chosen for its ability to lift objects to a higher height at a lower material cost. The arm was powered by two servo motors as the elbow and wrist joint, and two stepper motors at the shoulder and base rotation joint, as shown below. This is because we found from testing that the base and shoulder joints required a larger torque that was beyond the servo motors, therefore steppers were used. This arm was integrated with the three-pronged gripper and controlled by an Arduino microcontroller. The design of the arm focused on achieving precise and smooth movements while maintaining stability by not overextending the arm and created too much of a moment on the base. The use of stepper motors allowed for accurate positioning, which was essential since we were pursuing autonomous operation.\r\n\r\n![Robot Arm Manipulator Designs](../images/robotArmManipulatorDesign.png)\r\n\r\n## Controls\r\n\r\nBased on the design of the manipulator and gripper, the project requires 2 stepper motors and 3 servo motors. The electrical and software design was then based on controlling these components to achieve the desired engineering specifications. The motors would be controlled by either a microcontroller such as an Arduino or computer such as a RaspberryPi. A Raspberry Pi would have been ideal as its increased computing power could be used for the computer vision aspect of the project.\r\n\r\nHowever, Raspberry Pi’s were quite expensive at the time, so an Arduino was selected to be paired with a laptop to process the computer vision. A 5V 10A power supply was sourced and set as the foundation to source our motors upon. The provided servo motors from the University of Waterloo, the T9005 motors, were used since they met the required ratings to operate with the power supply. For stepper motors, two NEMA 17 motors were sourced from the University of Waterloo.\r\n\r\n![Robot Arm Circuit Diagram](../images/robotArmCircuitSchematic.png)\r\n\r\nA breadboard with MOM/OFF switches was attached to the Arduino to test each of the motors in the circuit. The circuit and motors were tested by confirming that you could turn each of them their full range of motion - 180 degrees for the servo motors, and indefinitely for stepper motors - in both directions after one another. This round of testing was successful in the first attempt in terms of the electrical circuit; however, it yielded important design decisions in terms of initializing the motors in Arduino’s software. \r\n\r\nFor the servo motors, the testing demonstrated that while each can travel 180 degrees, the software value for 0 and 180 degrees is different for each servo motor. The Adafruit Servo Shield Library recommended a minimum and maximum range of 100 and 500, respectively. Therefore, to find the actual range the range was increased by 5-10 on both ends until the servo motors were able to travel a range of 180 degrees. The final range for each of the servo motors is shown below in Table 3.\r\n\r\n![Robot Arm Circuit](../images/robotArmBreadboardCircuit.png)\r\n\r\n### Object Detection\r\n\r\nIn terms of object detection, the main design decisions were which sensors would be used and what corresponding software would be used to consistently detect the desired object. The main options for sensors were one overhead camera, two or more cameras set up around the manipulator, or ultrasonic sensors set up around the manipulator. As shown below, in Table 6, one overhead camera was selected due to its lower cost and complexity.\r\n\r\n## Software \r\n\r\nThis flow was modeled based on the functional requirements where the functional requirement is an action/event in the software. Additionally, the flow is a closed loop that returns to its home position to match Arduino’s main loop() function properties.\r\n\r\n![Robot Arm Software](../images/robotArmSoftwareFlow.png)\r\n\r\n![Robot Arm Software](../images/robotArmKinematics.png)\r\n\r\n## Object Detection\r\n\r\nWhen researching potential object detection protocol for the manipulator, it was determined that it would be too computationally intensive for the Arduino to perform all of the tasks. Therefore, the object detection protocol and conversion to motor positions would be completed on an external sensor and computer, with the final position for each of the motors being sent to the Arduino to control the motors. The general flow chart was updated to match this architecture and is shown below in Figure 8.\r\n\r\nThese coordinates would then be used to determine the angle of each joint of the manipulator, and therefore the position of all of the motors. The camera would be placed behind the manipulator’s base making the manipulator approximately the (0,0) coordinate. The general kinematics equation to solve for joint angles is shown below in equations 1-4 [2].\r\n\r\nThe software object detection algorithm that is the simplest to pair with the single overhead webcam while still providing accurate results is based on HSV contrast. The program would loop through the camera’s frames, moving noise can change the format from RGB to HSV. Afterwards, a mask of the desired colour in HSV is applied to the frame and the largest area is contoured. Finally, this contour is used to determine the center coordinates of the object. \r\n\r\n\r\n![Robot Arm HSV POC](../images/robotArmObjectDetect.jpg)\r\n\r\n\r\n# Conclusion\r\n\r\nThis final design was able to successfully satisfy all of the engineering design specifications listed in Table 1. With the use of a camera on a camera stand, the object detection is able to locate and record the benchy’s position and send these coordinates to the computer.  The design also demonstrates the ability to autonomously pick up and drop off target objects once its coordinates have been detected. This was demonstrated through hard-coding the robot to move to a pre-established pick-up location, grab the object, move the object to the required drop-off zone, and consequently release the object. Also, the manipulator and gripper can be manually controlled using the computer keyboard key for instances where the object has not been placed in a pre-set location.  This mode can be used to highlight the robot’s ability to reach objects further away and alternate between scooping and gripping pick-up methods. Full testing and results of this product can be found in the subsequent section.","src/data/projects/pick-and-place.md",[195,196,197,198,199,200,201,202],"../images/robotArmEngSpec.png","../images/robotArmGripperDesigns.png","../images/robotArmManipulatorDesign.png","../images/robotArmCircuitSchematic.png","../images/robotArmBreadboardCircuit.png","../images/robotArmSoftwareFlow.png","../images/robotArmKinematics.png","../images/robotArmObjectDetect.jpg","095a861f3e6a1216",{"html":205,"metadata":206},"\u003Ch1 id=\"overview\">Overview\u003C/h1>\n\u003Cp>My favourite course during my 3rd year at the University of Waterloo was definitely ME380 – Mechanical Engineering Design Workshop. The course was based on a term project where each group of students had to design and build a pick-and-place manipulator following specific constraints and requirements. I really enjoyed how this course pushed groups through each phase of the engineering design process and included the construction of a working prototype. It was also really interesting to see the different groups’ final manipulators and showcasing how group successfully tackled the constraints and requirements in a different manner.\u003C/p>\n\u003Cp>Our team chose to design a manipulator that was also autonomous and capable of dealing objects with complex geometries. This meant the manipulator needed to detect objects, calculate required movements, and handle objects ofvarious shapes and sizes, specifically those that are difficult to grasp. For the project, I was mainly focused on the electric and firmware aspects of the manipulator, including computer vision and motor control. However, everyone on the team wore many hats and was involved in all major design decisions.\u003C/p>\n\u003Ch2 id=\"learnings\">Learnings\u003C/h2>\n\u003Ch1 id=\"needs-analysis\">Needs Analysis\u003C/h1>\n\u003Cp>A need exists for a manipulator to lift and move a complex geometry object from a specified pick-up location to a specified drop-off zone autonomously without user intervention. The system must also feature the capability to handle adversarial objects.\u003C/p>\n\u003Ch2 id=\"design-specification\">Design Specification\u003C/h2>\n\u003Cp>The design constraints and criteria related to the course requirements and our team’s own desire for autonomous and adversarial grasps is shown in the table below.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/robotArmEngSpec.png&#x22;,&#x22;alt&#x22;:&#x22;Robot Arm Engineering Spec&#x22;,&#x22;index&#x22;:0}\">\u003C/p>\n\u003Ch1 id=\"design-process\">Design Process\u003C/h1>\n\u003Ch2 id=\"gripper-design\">Gripper Design\u003C/h2>\n\u003Cp>We explored three initial concepts for the gripper, with the focus on being able to pick up challenging and adversarial objects. Each of the designed grippers were to be powered by a micro-servo motor which was mounted into a rack-and-pinion system to translate a cylindrical rod that would open and close the gripper’s prongs through attached levers. All components of the gripper were 3D-printed except for the rack which was aluminum-based, the servo motor, and the bolts to mount the servo motor, this was to reduce the final moment arm of the gripper on the base of the manipulator for easier operation.\u003C/p>\n\u003Cp>The final design was a three-pronged gripper with an angled fin edge. This was chosen for its adaptability to various object geometries, as the fins would increase the contact area with the object and ensure grip by scooping the objects. The fins also acted as a cage which prevented the object from falling out of grip during operation of the manipulator – a common issue with the other designs.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/robotArmGripperDesigns.png&#x22;,&#x22;alt&#x22;:&#x22;Robot Arm Gripper Designs&#x22;,&#x22;index&#x22;:0}\">\u003C/p>\n\u003Ch2 id=\"manipulator-design\">Manipulator Design\u003C/h2>\n\u003Cp>Similarly, three designs were considered for the manipulator. This included a 4-DOF articulated arm, a crane-pulley system, and a 3D prismatic manipulator. Ultimately, the 4-DOF arm was chosen for its ability to lift objects to a higher height at a lower material cost. The arm was powered by two servo motors as the elbow and wrist joint, and two stepper motors at the shoulder and base rotation joint, as shown below. This is because we found from testing that the base and shoulder joints required a larger torque that was beyond the servo motors, therefore steppers were used. This arm was integrated with the three-pronged gripper and controlled by an Arduino microcontroller. The design of the arm focused on achieving precise and smooth movements while maintaining stability by not overextending the arm and created too much of a moment on the base. The use of stepper motors allowed for accurate positioning, which was essential since we were pursuing autonomous operation.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/robotArmManipulatorDesign.png&#x22;,&#x22;alt&#x22;:&#x22;Robot Arm Manipulator Designs&#x22;,&#x22;index&#x22;:0}\">\u003C/p>\n\u003Ch2 id=\"controls\">Controls\u003C/h2>\n\u003Cp>Based on the design of the manipulator and gripper, the project requires 2 stepper motors and 3 servo motors. The electrical and software design was then based on controlling these components to achieve the desired engineering specifications. The motors would be controlled by either a microcontroller such as an Arduino or computer such as a RaspberryPi. A Raspberry Pi would have been ideal as its increased computing power could be used for the computer vision aspect of the project.\u003C/p>\n\u003Cp>However, Raspberry Pi’s were quite expensive at the time, so an Arduino was selected to be paired with a laptop to process the computer vision. A 5V 10A power supply was sourced and set as the foundation to source our motors upon. The provided servo motors from the University of Waterloo, the T9005 motors, were used since they met the required ratings to operate with the power supply. For stepper motors, two NEMA 17 motors were sourced from the University of Waterloo.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/robotArmCircuitSchematic.png&#x22;,&#x22;alt&#x22;:&#x22;Robot Arm Circuit Diagram&#x22;,&#x22;index&#x22;:0}\">\u003C/p>\n\u003Cp>A breadboard with MOM/OFF switches was attached to the Arduino to test each of the motors in the circuit. The circuit and motors were tested by confirming that you could turn each of them their full range of motion - 180 degrees for the servo motors, and indefinitely for stepper motors - in both directions after one another. This round of testing was successful in the first attempt in terms of the electrical circuit; however, it yielded important design decisions in terms of initializing the motors in Arduino’s software.\u003C/p>\n\u003Cp>For the servo motors, the testing demonstrated that while each can travel 180 degrees, the software value for 0 and 180 degrees is different for each servo motor. The Adafruit Servo Shield Library recommended a minimum and maximum range of 100 and 500, respectively. Therefore, to find the actual range the range was increased by 5-10 on both ends until the servo motors were able to travel a range of 180 degrees. The final range for each of the servo motors is shown below in Table 3.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/robotArmBreadboardCircuit.png&#x22;,&#x22;alt&#x22;:&#x22;Robot Arm Circuit&#x22;,&#x22;index&#x22;:0}\">\u003C/p>\n\u003Ch3 id=\"object-detection\">Object Detection\u003C/h3>\n\u003Cp>In terms of object detection, the main design decisions were which sensors would be used and what corresponding software would be used to consistently detect the desired object. The main options for sensors were one overhead camera, two or more cameras set up around the manipulator, or ultrasonic sensors set up around the manipulator. As shown below, in Table 6, one overhead camera was selected due to its lower cost and complexity.\u003C/p>\n\u003Ch2 id=\"software\">Software\u003C/h2>\n\u003Cp>This flow was modeled based on the functional requirements where the functional requirement is an action/event in the software. Additionally, the flow is a closed loop that returns to its home position to match Arduino’s main loop() function properties.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/robotArmSoftwareFlow.png&#x22;,&#x22;alt&#x22;:&#x22;Robot Arm Software&#x22;,&#x22;index&#x22;:0}\">\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/robotArmKinematics.png&#x22;,&#x22;alt&#x22;:&#x22;Robot Arm Software&#x22;,&#x22;index&#x22;:0}\">\u003C/p>\n\u003Ch2 id=\"object-detection-1\">Object Detection\u003C/h2>\n\u003Cp>When researching potential object detection protocol for the manipulator, it was determined that it would be too computationally intensive for the Arduino to perform all of the tasks. Therefore, the object detection protocol and conversion to motor positions would be completed on an external sensor and computer, with the final position for each of the motors being sent to the Arduino to control the motors. The general flow chart was updated to match this architecture and is shown below in Figure 8.\u003C/p>\n\u003Cp>These coordinates would then be used to determine the angle of each joint of the manipulator, and therefore the position of all of the motors. The camera would be placed behind the manipulator’s base making the manipulator approximately the (0,0) coordinate. The general kinematics equation to solve for joint angles is shown below in equations 1-4 [2].\u003C/p>\n\u003Cp>The software object detection algorithm that is the simplest to pair with the single overhead webcam while still providing accurate results is based on HSV contrast. The program would loop through the camera’s frames, moving noise can change the format from RGB to HSV. Afterwards, a mask of the desired colour in HSV is applied to the frame and the largest area is contoured. Finally, this contour is used to determine the center coordinates of the object.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/robotArmObjectDetect.jpg&#x22;,&#x22;alt&#x22;:&#x22;Robot Arm HSV POC&#x22;,&#x22;index&#x22;:0}\">\u003C/p>\n\u003Ch1 id=\"conclusion\">Conclusion\u003C/h1>\n\u003Cp>This final design was able to successfully satisfy all of the engineering design specifications listed in Table 1. With the use of a camera on a camera stand, the object detection is able to locate and record the benchy’s position and send these coordinates to the computer.  The design also demonstrates the ability to autonomously pick up and drop off target objects once its coordinates have been detected. This was demonstrated through hard-coding the robot to move to a pre-established pick-up location, grab the object, move the object to the required drop-off zone, and consequently release the object. Also, the manipulator and gripper can be manually controlled using the computer keyboard key for instances where the object has not been placed in a pre-set location.  This mode can be used to highlight the robot’s ability to reach objects further away and alternate between scooping and gripping pick-up methods. Full testing and results of this product can be found in the subsequent section.\u003C/p>",{"headings":207,"localImagePaths":237,"remoteImagePaths":238,"frontmatter":239,"imagePaths":242},[208,209,210,213,216,219,222,225,228,231,232,234],{"depth":40,"slug":41,"text":42},{"depth":130,"slug":83,"text":84},{"depth":40,"slug":211,"text":212},"needs-analysis","Needs Analysis",{"depth":130,"slug":214,"text":215},"design-specification","Design Specification",{"depth":40,"slug":217,"text":218},"design-process","Design Process",{"depth":130,"slug":220,"text":221},"gripper-design","Gripper Design",{"depth":130,"slug":223,"text":224},"manipulator-design","Manipulator Design",{"depth":130,"slug":226,"text":227},"controls","Controls",{"depth":156,"slug":229,"text":230},"object-detection","Object Detection",{"depth":130,"slug":143,"text":61},{"depth":130,"slug":233,"text":230},"object-detection-1",{"depth":40,"slug":235,"text":236},"conclusion","Conclusion",[195,196,197,198,199,200,201,202],[],{"project_id":184,"title":185,"description":186,"categories":240,"tags":241,"image":190,"date_string":191},[61,60],[63,102,103,189],[195,196,197,198,199,200,201,202],"quadcopter",{"id":243,"data":245,"body":252,"filePath":253,"assetImports":254,"digest":259,"rendered":260},{"project_id":130,"title":246,"description":247,"categories":248,"tags":249,"image":250,"date_string":251},"Arduino Quadcopter","A self-balancing quadcopter powered by arduino, gyro, and brushless motors",[60,61],[102,103,23,24],"src/data/images/coverQuad.png","Summer 2021","# Overview\r\n\r\nAs I was exposed to new aspects of engineering from my co-ops, I realized that I was very unfamiliar with electronics, firmware, and printed circuit boards (PCBs). Since I would not get much exposure to these concepts during my mechanical engineering degree, I decided to build an arduino-powered quadcopter. I completed this project during the 2020 summer and fall term.\r\n\r\nAs a starting point, I found an electrical schematic for an arduino-powered quadcopter online, shown to the right, and based my project on it. I wanted to avoid pre-built drone kits during so that I could really learn the engineering concepts and experience sourcing components for a project as if I was at a start-up or company.\r\n\r\n![Quadcopter reference electrical schematic](../images/quadCircuitSchema.jpg)\r\n*Quadcopter reference electrical schematic*\r\n\r\nI sourced all the components from various suppliers such as Amazon, Abra Electronics, and Digikey. Since I was sourcing components individually and not buying a whole kit, it was imperative that I ensure all of the components would connect properly. A table and picture of all of the components are shown below. I also purchased a soldering kit, variety resistor pack, and male-to-male breadboard jumper cables.\r\n\r\n![Quadcopter components](../images/quadComponents.jpg)\r\n*Quadcopter components*\r\n\r\n## Assembly\r\n\r\nsoldering\r\n\r\nOnce the bottom frame was assemebled, I switched my foucs to connecting the arduino and gyro to the assembly. Following the aforementioned schematic, I first soldered my wires to the gyroscope. I connected the I2C SDA & SCL wires to the corresponding ports on the Arduino Uno, and the ground and power to the bottom frame of the quadcopter. I connected two resistors (1.5k and 1k) in series and used them between the connection with the battery and arduino's VIN port. When connecting the arduino to the ESCs the specific pins used were noted from programming the controls in the next step. The remainng wires were integrated following the schematic. Finally, the transmitter was mounted to the lower frame using double sided tape.\r\n\r\n![Connection between Arduino and Gyro](../images/quadCircuitBoard.png)\r\n*Connection between Arduino and Gyro*\r\n\r\n## Software\r\n\r\nOnce assembly was finished, the next step was setting up the software and controls of the quadcopter. The biggest hurdle for me during this process was understanding Pin change interrupts. Pin change interrupts interrupt a running program when the state of a digital pin chagnes. For the Arduino Uno the pin change interrupt will trigger if the D8, D9, D10, D11, D12 pins were toggled. Therefore, the receiver inputs for the transmitter were connected to pins D8-D11. In terms of software, this meant enabling the pin change interrupt by setting the PCIE0 bit to one, and enabling each digital pin individually by the PCMSK0 register. Once the interrupts were enabled, I researched and figured out that I needed to develop an interupt routine that would execute everytime one of the receiver pins changed states. This subroutine would check if each input switched on/off; remember the state; and adjust that pins timer accordingly. I forked a software library online that would use these input states to determine the yaw, roll and pitch of the quadcopter based on these inputs and set the ESC's accordingly.\r\n\r\nThe final step of the quadcopter proejct was integrating the gyro into the system. I was using the L3G4200D gyro; based on its documentation, I found that its output was an angular rate sensor. The registers that i used in this project were CTRL_REG1, CTRL_REG4, and the outputs for the x, y, and z axis. I needed to set the BDU bit of CTRL_REG4 since the angular rate data is sent through two bytes, and we need to be sure both bytes are from the same semple time. During operation, the arduino is reading angualr rate data from the register addresses of the x, y, and z registers. Since I would be requesting 6 bytes of data (2 for each axis), I programmed the Arduino to wait until all 6 bytes have been received before reading its values.\r\n\r\nThe final task of the quadcopter project was calibrating the ESC's for flight. The ESCS are controlled with a 1000us till 2000us pulse - 1000 being off, and 2000 being full throttle. Knowing this, the main software loop became:\r\n\r\n    Read the gyro angular rate data\r\n    Calculate the PID corrections\r\n    Calculate the puls for every sec\r\n    Compensate every pulse for voltage drop\r\n    Send the Calculated pulses (1000-2000) to the ESCs\r\n\r\n# Summary\r\n\r\nIn summary, this project was a great learning experience that allowed be to better understand electronics and firmware. I was actually surpised how much I learned in terms of programming PCB registers and motor controls. This project was very helpful when I was working on the Hardware-In-Loop test fixture during my co-op at Electrans.\r\n\r\n![Quadcopter cover photo](../images/coverQuad.png)","src/data/projects/quadcopter.md",[255,256,257,258],"../images/quadCircuitSchema.jpg","../images/quadComponents.jpg","../images/quadCircuitBoard.png","../images/coverQuad.png","9dc21cbd4bf58cf7",{"html":261,"metadata":262},"\u003Ch1 id=\"overview\">Overview\u003C/h1>\n\u003Cp>As I was exposed to new aspects of engineering from my co-ops, I realized that I was very unfamiliar with electronics, firmware, and printed circuit boards (PCBs). Since I would not get much exposure to these concepts during my mechanical engineering degree, I decided to build an arduino-powered quadcopter. I completed this project during the 2020 summer and fall term.\u003C/p>\n\u003Cp>As a starting point, I found an electrical schematic for an arduino-powered quadcopter online, shown to the right, and based my project on it. I wanted to avoid pre-built drone kits during so that I could really learn the engineering concepts and experience sourcing components for a project as if I was at a start-up or company.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/quadCircuitSchema.jpg&#x22;,&#x22;alt&#x22;:&#x22;Quadcopter reference electrical schematic&#x22;,&#x22;index&#x22;:0}\">\r\n\u003Cem>Quadcopter reference electrical schematic\u003C/em>\u003C/p>\n\u003Cp>I sourced all the components from various suppliers such as Amazon, Abra Electronics, and Digikey. Since I was sourcing components individually and not buying a whole kit, it was imperative that I ensure all of the components would connect properly. A table and picture of all of the components are shown below. I also purchased a soldering kit, variety resistor pack, and male-to-male breadboard jumper cables.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/quadComponents.jpg&#x22;,&#x22;alt&#x22;:&#x22;Quadcopter components&#x22;,&#x22;index&#x22;:0}\">\r\n\u003Cem>Quadcopter components\u003C/em>\u003C/p>\n\u003Ch2 id=\"assembly\">Assembly\u003C/h2>\n\u003Cp>soldering\u003C/p>\n\u003Cp>Once the bottom frame was assemebled, I switched my foucs to connecting the arduino and gyro to the assembly. Following the aforementioned schematic, I first soldered my wires to the gyroscope. I connected the I2C SDA &#x26; SCL wires to the corresponding ports on the Arduino Uno, and the ground and power to the bottom frame of the quadcopter. I connected two resistors (1.5k and 1k) in series and used them between the connection with the battery and arduino’s VIN port. When connecting the arduino to the ESCs the specific pins used were noted from programming the controls in the next step. The remainng wires were integrated following the schematic. Finally, the transmitter was mounted to the lower frame using double sided tape.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/quadCircuitBoard.png&#x22;,&#x22;alt&#x22;:&#x22;Connection between Arduino and Gyro&#x22;,&#x22;index&#x22;:0}\">\r\n\u003Cem>Connection between Arduino and Gyro\u003C/em>\u003C/p>\n\u003Ch2 id=\"software\">Software\u003C/h2>\n\u003Cp>Once assembly was finished, the next step was setting up the software and controls of the quadcopter. The biggest hurdle for me during this process was understanding Pin change interrupts. Pin change interrupts interrupt a running program when the state of a digital pin chagnes. For the Arduino Uno the pin change interrupt will trigger if the D8, D9, D10, D11, D12 pins were toggled. Therefore, the receiver inputs for the transmitter were connected to pins D8-D11. In terms of software, this meant enabling the pin change interrupt by setting the PCIE0 bit to one, and enabling each digital pin individually by the PCMSK0 register. Once the interrupts were enabled, I researched and figured out that I needed to develop an interupt routine that would execute everytime one of the receiver pins changed states. This subroutine would check if each input switched on/off; remember the state; and adjust that pins timer accordingly. I forked a software library online that would use these input states to determine the yaw, roll and pitch of the quadcopter based on these inputs and set the ESC’s accordingly.\u003C/p>\n\u003Cp>The final step of the quadcopter proejct was integrating the gyro into the system. I was using the L3G4200D gyro; based on its documentation, I found that its output was an angular rate sensor. The registers that i used in this project were CTRL_REG1, CTRL_REG4, and the outputs for the x, y, and z axis. I needed to set the BDU bit of CTRL_REG4 since the angular rate data is sent through two bytes, and we need to be sure both bytes are from the same semple time. During operation, the arduino is reading angualr rate data from the register addresses of the x, y, and z registers. Since I would be requesting 6 bytes of data (2 for each axis), I programmed the Arduino to wait until all 6 bytes have been received before reading its values.\u003C/p>\n\u003Cp>The final task of the quadcopter project was calibrating the ESC’s for flight. The ESCS are controlled with a 1000us till 2000us pulse - 1000 being off, and 2000 being full throttle. Knowing this, the main software loop became:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>Read the gyro angular rate data\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Calculate the PID corrections\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Calculate the puls for every sec\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Compensate every pulse for voltage drop\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Send the Calculated pulses (1000-2000) to the ESCs\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch1 id=\"summary\">Summary\u003C/h1>\n\u003Cp>In summary, this project was a great learning experience that allowed be to better understand electronics and firmware. I was actually surpised how much I learned in terms of programming PCB registers and motor controls. This project was very helpful when I was working on the Hardware-In-Loop test fixture during my co-op at Electrans.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/coverQuad.png&#x22;,&#x22;alt&#x22;:&#x22;Quadcopter cover photo&#x22;,&#x22;index&#x22;:0}\">\u003C/p>",{"headings":263,"localImagePaths":272,"remoteImagePaths":273,"frontmatter":274,"imagePaths":277},[264,265,268,269],{"depth":40,"slug":41,"text":42},{"depth":130,"slug":266,"text":267},"assembly","Assembly",{"depth":130,"slug":143,"text":61},{"depth":40,"slug":270,"text":271},"summary","Summary",[255,256,257,258],[],{"project_id":130,"title":246,"description":247,"categories":275,"tags":276,"image":250,"date_string":251},[60,61],[102,103,23,24],[255,256,257,258],"carbon-greenfoot-tracker",{"id":278,"data":280,"body":290,"filePath":291,"assetImports":292,"digest":300,"rendered":301},{"project_id":40,"title":281,"description":282,"categories":283,"tags":284,"image":288,"date_string":289},"Carbon Footprint Tracker","A social media platform for users to carbon footprint from travel and \"race\" others users for the lowest weekly footprint",[61],[285,286,287],"Django (Python)","HTML & CSS","Javascript","src/data/images/coverCftracker.png","Winter 2021","# Overiew\r\n\r\n On August 9th, 2021, the Intergovernmental Panel on Climate Change (IPCC) released the first part of their sixth assessment of the climate crisis. The report highlighted how human activity has increased the global average surface temperate by 1.50C since 1850. The report also highlighted potential climate scenarios that could occur; as the temperature increased with each scenario, climate extremes, such as heat waves, heavy precipitation, droughts, and tropical cyclones became increasingly frequent. The reports central conclusion to limiting future human-induced global warming requires limiting cumulative carbon dioxide emissions. This is because scientists have found a near-linear link between cumulative anthropogenic CO2 emissions and the global warming they cause.\r\n\r\nThe objective of the project is to design and build a web application that incentivizes users to track their carbon dioxide emissions to increase their self-awareness of their emissions and make lifestyle changes to limit global warming. The project, named GreenFoot, must allow users to track their carbon footprint, input how far they travelled with what method to calculate carbon dioxide emissions, and add friends to compare weekly carbon dioxide emissions. GreenFoot was built using a MySQL relational database, Django (Python) backend, and a HTML, CSS & JavaScript frontend. The code for this project can be found on my Github here.\r\n\r\nOutside of this report, this project was submitted as a final capstone project in the University of Harvard’s CS50 Web Programming Course with Python and JavaScript. The requirements from the course were that the project be an original, complex web application that uses an SQL Database, Django (Python) back-end, JavaScript front-end, and be mobile responsive.\r\n\r\n## Background Information\r\n\r\nOn August 9th, 2021, the Intergovernmental Panel on Climate Change (IPCC) released the first part of their sixth assessment of the climate crisis. The report was divided into four parts: the current state of the climate, possible climate futures, climate information for risk assessment per region, and limiting future climate change.\r\n\r\nThe first section, the current state of the climate, presented how human activity has affected the earth’s climate and is causing both current and future consequences. Human activity has increased average global surface temperatures by approximately 1.50C since 1850. The first section of the report also highlighted the current consequences of global warming. Globally averaged precipitation has likely increased since 1950. There are observed changes in near-surface ocean salinity, mid-latitude storms, and the global retreat of glaciers since the 1990s. Global mean sea level has increase by 0.20 m between 1901 and 2018. Climate zones have shifted poleward in both hemispheres. Weather and climate extremes, such as “heat waves, heavy precipitation, droughts, and tropical cyclones”, have become increasingly frequent. It is “extremely likely” that each of these anomalies have human activity as the main driver.\r\n\r\n![Change in surface temperatures](../images/cftrackerGraph1.png)\r\n*Change in global surface temperature (annual average) as observed and simulated using human & natural and natural factors*\r\n\r\nThe second section of the report, possible climate futures, examined five potential scenarios that could occur based the potential differences in global surface temperature between 1850-1900 to 2081-2100. The five scenarios are SSP1-1.9, SSP1-2.6, SSP2-4.5, SSP3-7.0 and SSP5-8.5 and their differences are 1.0 to 1.80C, 1.3 to 2.40C, 2.1 to 3.50C, 2.8 to 4.60C, 3.3 to 5.70C, respectively. Unfortunately, despite these varying temperature differences, each scenario has global surface temperatures increase until at least the mid-century. Additionally, many climate changes from past and future greenhouse gas emissions are “irreversible for centuries to millennia, especially changes in the ocean, ice sheets and global sea level”. When examining each of the scenarios, there is a direct relation between the level of changes in a climate system and the increase in global warming. In other words, the worse the scenario, the greater the frequency and intensity of hot extremes, marine heatwaves, heavy precipitation, agricultural and ecological droughts, and more.\r\n\r\n![Co2 emissions and surface temperature relation](../images/cftrackerGraph2.png)\r\n*Near-linear relationship between cumulative CO2 emissions and the increase in global surface temperature.*\r\n\r\nThe reports central conclusion to limiting future human-induced global warming requires limiting cumulative carbon dioxide emissions - reaching at least net zero emissions. This is because scientists have found a near-linear link between cumulative anthropogenic CO2 emissions and the global warming they cause.\r\n\r\n# Solution: Greenfoot\r\n\r\n## Problem Definition\r\n\r\nThe IPCC’s report describes a clear link between global warming and increasing carbon dioxide emissions, and that these emissions are coming from an increase in human activity. The transportation sector represents the largest share of greenhouse gas emissions with 29% of total emissions. This is from the burning of fossil fuels for vehicles such as cars, trucks, and planes. Therefore, it is likely that humans can lower their carbon dioxide emissions by reducing the amount they travel or altering their transportation methods. A study in New Zealand examined climate change awareness and its link with travel related decision-making found that climate change awareness does not appear to influence travel-related decisions. Unfortunately, this study was limited to aviation travel and did not examine daily travel with cars, trucks, trains, etc. Therefore, citizens need a method to exactly see how much carbon dioxide they are emitting with their travels in order to reflect and make a lifestyle change.\r\n\r\n### Functional Requirements\r\n\r\nFirst, the user must be able to view their carbon dioxide emissions, henceforth carbon footprint. The user should be able to view both their total carbon footprint and their carbon footprint on a smaller scope such as per week or month.\r\n\r\nSecond, users must be able to input how far they travelled (km/h) and their method of travel. Once inputted, the web app will calculate the amount of carbon dioxide emitted based on the method of travel and add that value to their carbon footprint. This allows users to consistently track their emissions every time they travel.\r\n\r\nThird, the user should be able to compete in weekly bouts with their friends to see which user has the lowest carbon footprint for the week. These races will socially incentivize users to decrease their carbon footprint while having fun competing with friends. This function also creates a sub-function within the web app to allow users to add or remove other users as friends.\r\n\r\nFinally, the web app needs to have an authentication form for users to sign in or create an account. This allows each user to have their own saved profile that they can access from any network or device.\r\n\r\n### Constraints and Criteria\r\n\r\n The constraints and criteria of a project are imperative, as they not only ensure the project’s design encompasses a client’s vision, but also create a means to allow the comparison between other possible designs.\r\n\r\nThe constraints represent requirements that must be met in order for a design to be successful. First, the web app must have a simplistic design. A simple design allows users to quickly understand how to use the web app. Therefore, each page of the web app should only have on main function; for instance, the input page should only have the form to input the distance travelled and method of travel. Secondly, the web application must be secure from web attacks. Since the web application handles a user’s progress as well as their personal information such as email addresses, it is imperative that the application be secure to protect that information.\r\n\r\nThe criteria of a project act as a guide to its completed state, as they are typically measurable attributes. The only criterion for this web app is for it to be responsive to all devices – including mobile. This constraint is imposed by Harvard’s CS50 Web Programming Course with Python and JavaScript. \r\n\r\n## Design\r\n\r\nWhen designing GreenFoot, the main functions, constraint, and criteria acted as guidelines. To maintain a simple design, a unique page was created for each requirement: an Input page for users to input their travels, a Profile Page for users to see their total and weekly carbon footprint, a Friends page where users can add or remove other users as friends, a Races page where users can view all races and also request to start a race with a friend, and a unique Race page for each race. Each page will be accessible to each other with a common nav bar at the top of each page. Additionally, to add a first layer of security, there will be an initial sign-in or sign-up page to authenticate users and protect their account.\r\n\r\n| Page | Function(s)  |\r\n|---|---|\r\n| Input  | - Input form for users to submit how far they travelled and their method of travel\r\n            - Calculate gallons of CO2 per km based on form submission and add to profile and the user’s races  |\r\n| Profile  | - Display total emissions by user\r\n             - Display weekly emmisions by user |\r\n| Friends  | - Display all users\r\n             - If the user is a friend, current user can remove the user as a friend\r\n             - If the user is not a friend, current user can send the user a friend request |\r\n| Race list | - Display all races, user can click a race to be taken to its race page\r\n              - Send a race reqeust to a friend to start a race\r\n              - Accept or decline current user's race requests |\r\n| Race | - Show each users emissions for the race\r\n         - If the race is over, display the winner |\r\n\r\n\r\nA well-designed database dictates the robustness and scalability of its web application. The Harvard course requires that the web application use a SQL relational database. Therefore, GreenFoot database will have related tables, known as models. The essential models for GreenFoot are User, Race, and Footprint. The User model will have fields like username, email, password, and friends. The friends will be an SQL many-to-many relationship between other User models. The Race model will have a timestamp, two users, and an end date. Finally, the Footprint model will have a total emissions integer field, a start date to track their point in the week, and seven integer fields for the emission of each the day of the week.\r\n\r\nThe Django framework that was used in the project is a Model-Template-View framework (MTV). The backend will query the database and render the required page based on the request made by the user. For instance, if the current user clicks the link to the Profile page on the navbar, the request will be sent to the server and the app will run the function specific to the profile view and send the appropriate data (HTML & CSS, JavaScript) to the client. The Input page has two functions based on the type of request. A get request has the web app create two forms with Django Form library and renders those forms to the client. A post request has the web app acquire the data sent from the client, validates the data and then calculates the data to add it to the user’s carbon footprint.\r\n\r\nGreenfoot’s frontend was developed with HTML & CSS, and vanilla JavaScript. No framework was used since the Harvard course only taught vanilla JavaScript. The frontend also used Bootstrap, an open-source CSS library, since its design functionality allows for easier mobile-responsiveness. For simple designs, each page will have a nav bar on top to link each of the pages together. One of the main aspects is the display of weekly carbon footprint. The display could be completed with a line graph, pie chart, or histogram. It was decided to use a histogram to give a clean and simple comparison as well as show a separation between the days. This histogram was created using the Chart JS open-source library.\r\n\r\n## Final Result\r\n\r\n![Input Page](../images/cftrackerInput.png)*Input Page*\r\n![Friends Page](../images/cftrackerFriends.png)*Friends Page*\r\n![Race Page](../images/cftrackerShow.png)*Race Page*\r\n![Racelist Page](../images/cftrackerRaces.png)*Race list Page*\r\n![Profile Page](../images/coverCftracker.png)*Profiile page*","src/data/projects/carbon-greenfoot-tracker.md",[293,294,295,296,297,298,299],"../images/cftrackerGraph1.png","../images/cftrackerGraph2.png","../images/cftrackerInput.png","../images/cftrackerFriends.png","../images/cftrackerShow.png","../images/cftrackerRaces.png","../images/coverCftracker.png","434c25f2b26b4c54",{"html":302,"metadata":303},"\u003Ch1 id=\"overiew\">Overiew\u003C/h1>\n\u003Cp>On August 9th, 2021, the Intergovernmental Panel on Climate Change (IPCC) released the first part of their sixth assessment of the climate crisis. The report highlighted how human activity has increased the global average surface temperate by 1.50C since 1850. The report also highlighted potential climate scenarios that could occur; as the temperature increased with each scenario, climate extremes, such as heat waves, heavy precipitation, droughts, and tropical cyclones became increasingly frequent. The reports central conclusion to limiting future human-induced global warming requires limiting cumulative carbon dioxide emissions. This is because scientists have found a near-linear link between cumulative anthropogenic CO2 emissions and the global warming they cause.\u003C/p>\n\u003Cp>The objective of the project is to design and build a web application that incentivizes users to track their carbon dioxide emissions to increase their self-awareness of their emissions and make lifestyle changes to limit global warming. The project, named GreenFoot, must allow users to track their carbon footprint, input how far they travelled with what method to calculate carbon dioxide emissions, and add friends to compare weekly carbon dioxide emissions. GreenFoot was built using a MySQL relational database, Django (Python) backend, and a HTML, CSS &#x26; JavaScript frontend. The code for this project can be found on my Github here.\u003C/p>\n\u003Cp>Outside of this report, this project was submitted as a final capstone project in the University of Harvard’s CS50 Web Programming Course with Python and JavaScript. The requirements from the course were that the project be an original, complex web application that uses an SQL Database, Django (Python) back-end, JavaScript front-end, and be mobile responsive.\u003C/p>\n\u003Ch2 id=\"background-information\">Background Information\u003C/h2>\n\u003Cp>On August 9th, 2021, the Intergovernmental Panel on Climate Change (IPCC) released the first part of their sixth assessment of the climate crisis. The report was divided into four parts: the current state of the climate, possible climate futures, climate information for risk assessment per region, and limiting future climate change.\u003C/p>\n\u003Cp>The first section, the current state of the climate, presented how human activity has affected the earth’s climate and is causing both current and future consequences. Human activity has increased average global surface temperatures by approximately 1.50C since 1850. The first section of the report also highlighted the current consequences of global warming. Globally averaged precipitation has likely increased since 1950. There are observed changes in near-surface ocean salinity, mid-latitude storms, and the global retreat of glaciers since the 1990s. Global mean sea level has increase by 0.20 m between 1901 and 2018. Climate zones have shifted poleward in both hemispheres. Weather and climate extremes, such as “heat waves, heavy precipitation, droughts, and tropical cyclones”, have become increasingly frequent. It is “extremely likely” that each of these anomalies have human activity as the main driver.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/cftrackerGraph1.png&#x22;,&#x22;alt&#x22;:&#x22;Change in surface temperatures&#x22;,&#x22;index&#x22;:0}\">\r\n\u003Cem>Change in global surface temperature (annual average) as observed and simulated using human &#x26; natural and natural factors\u003C/em>\u003C/p>\n\u003Cp>The second section of the report, possible climate futures, examined five potential scenarios that could occur based the potential differences in global surface temperature between 1850-1900 to 2081-2100. The five scenarios are SSP1-1.9, SSP1-2.6, SSP2-4.5, SSP3-7.0 and SSP5-8.5 and their differences are 1.0 to 1.80C, 1.3 to 2.40C, 2.1 to 3.50C, 2.8 to 4.60C, 3.3 to 5.70C, respectively. Unfortunately, despite these varying temperature differences, each scenario has global surface temperatures increase until at least the mid-century. Additionally, many climate changes from past and future greenhouse gas emissions are “irreversible for centuries to millennia, especially changes in the ocean, ice sheets and global sea level”. When examining each of the scenarios, there is a direct relation between the level of changes in a climate system and the increase in global warming. In other words, the worse the scenario, the greater the frequency and intensity of hot extremes, marine heatwaves, heavy precipitation, agricultural and ecological droughts, and more.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/cftrackerGraph2.png&#x22;,&#x22;alt&#x22;:&#x22;Co2 emissions and surface temperature relation&#x22;,&#x22;index&#x22;:0}\">\r\n\u003Cem>Near-linear relationship between cumulative CO2 emissions and the increase in global surface temperature.\u003C/em>\u003C/p>\n\u003Cp>The reports central conclusion to limiting future human-induced global warming requires limiting cumulative carbon dioxide emissions - reaching at least net zero emissions. This is because scientists have found a near-linear link between cumulative anthropogenic CO2 emissions and the global warming they cause.\u003C/p>\n\u003Ch1 id=\"solution-greenfoot\">Solution: Greenfoot\u003C/h1>\n\u003Ch2 id=\"problem-definition\">Problem Definition\u003C/h2>\n\u003Cp>The IPCC’s report describes a clear link between global warming and increasing carbon dioxide emissions, and that these emissions are coming from an increase in human activity. The transportation sector represents the largest share of greenhouse gas emissions with 29% of total emissions. This is from the burning of fossil fuels for vehicles such as cars, trucks, and planes. Therefore, it is likely that humans can lower their carbon dioxide emissions by reducing the amount they travel or altering their transportation methods. A study in New Zealand examined climate change awareness and its link with travel related decision-making found that climate change awareness does not appear to influence travel-related decisions. Unfortunately, this study was limited to aviation travel and did not examine daily travel with cars, trucks, trains, etc. Therefore, citizens need a method to exactly see how much carbon dioxide they are emitting with their travels in order to reflect and make a lifestyle change.\u003C/p>\n\u003Ch3 id=\"functional-requirements\">Functional Requirements\u003C/h3>\n\u003Cp>First, the user must be able to view their carbon dioxide emissions, henceforth carbon footprint. The user should be able to view both their total carbon footprint and their carbon footprint on a smaller scope such as per week or month.\u003C/p>\n\u003Cp>Second, users must be able to input how far they travelled (km/h) and their method of travel. Once inputted, the web app will calculate the amount of carbon dioxide emitted based on the method of travel and add that value to their carbon footprint. This allows users to consistently track their emissions every time they travel.\u003C/p>\n\u003Cp>Third, the user should be able to compete in weekly bouts with their friends to see which user has the lowest carbon footprint for the week. These races will socially incentivize users to decrease their carbon footprint while having fun competing with friends. This function also creates a sub-function within the web app to allow users to add or remove other users as friends.\u003C/p>\n\u003Cp>Finally, the web app needs to have an authentication form for users to sign in or create an account. This allows each user to have their own saved profile that they can access from any network or device.\u003C/p>\n\u003Ch3 id=\"constraints-and-criteria\">Constraints and Criteria\u003C/h3>\n\u003Cp>The constraints and criteria of a project are imperative, as they not only ensure the project’s design encompasses a client’s vision, but also create a means to allow the comparison between other possible designs.\u003C/p>\n\u003Cp>The constraints represent requirements that must be met in order for a design to be successful. First, the web app must have a simplistic design. A simple design allows users to quickly understand how to use the web app. Therefore, each page of the web app should only have on main function; for instance, the input page should only have the form to input the distance travelled and method of travel. Secondly, the web application must be secure from web attacks. Since the web application handles a user’s progress as well as their personal information such as email addresses, it is imperative that the application be secure to protect that information.\u003C/p>\n\u003Cp>The criteria of a project act as a guide to its completed state, as they are typically measurable attributes. The only criterion for this web app is for it to be responsive to all devices – including mobile. This constraint is imposed by Harvard’s CS50 Web Programming Course with Python and JavaScript.\u003C/p>\n\u003Ch2 id=\"design\">Design\u003C/h2>\n\u003Cp>When designing GreenFoot, the main functions, constraint, and criteria acted as guidelines. To maintain a simple design, a unique page was created for each requirement: an Input page for users to input their travels, a Profile Page for users to see their total and weekly carbon footprint, a Friends page where users can add or remove other users as friends, a Races page where users can view all races and also request to start a race with a friend, and a unique Race page for each race. Each page will be accessible to each other with a common nav bar at the top of each page. Additionally, to add a first layer of security, there will be an initial sign-in or sign-up page to authenticate users and protect their account.\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Page\u003C/th>\u003Cth>Function(s)\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>Input\u003C/td>\u003Ctd>- Input form for users to submit how far they travelled and their method of travel\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>        - Calculate gallons of CO2 per km based on form submission and add to profile and the user’s races  |\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>| Profile  | - Display total emissions by user\r\n- Display weekly emmisions by user |\r\n| Friends  | - Display all users\r\n- If the user is a friend, current user can remove the user as a friend\r\n- If the user is not a friend, current user can send the user a friend request |\r\n| Race list | - Display all races, user can click a race to be taken to its race page\r\n- Send a race reqeust to a friend to start a race\r\n- Accept or decline current user’s race requests |\r\n| Race | - Show each users emissions for the race\r\n- If the race is over, display the winner |\u003C/p>\n\u003Cp>A well-designed database dictates the robustness and scalability of its web application. The Harvard course requires that the web application use a SQL relational database. Therefore, GreenFoot database will have related tables, known as models. The essential models for GreenFoot are User, Race, and Footprint. The User model will have fields like username, email, password, and friends. The friends will be an SQL many-to-many relationship between other User models. The Race model will have a timestamp, two users, and an end date. Finally, the Footprint model will have a total emissions integer field, a start date to track their point in the week, and seven integer fields for the emission of each the day of the week.\u003C/p>\n\u003Cp>The Django framework that was used in the project is a Model-Template-View framework (MTV). The backend will query the database and render the required page based on the request made by the user. For instance, if the current user clicks the link to the Profile page on the navbar, the request will be sent to the server and the app will run the function specific to the profile view and send the appropriate data (HTML &#x26; CSS, JavaScript) to the client. The Input page has two functions based on the type of request. A get request has the web app create two forms with Django Form library and renders those forms to the client. A post request has the web app acquire the data sent from the client, validates the data and then calculates the data to add it to the user’s carbon footprint.\u003C/p>\n\u003Cp>Greenfoot’s frontend was developed with HTML &#x26; CSS, and vanilla JavaScript. No framework was used since the Harvard course only taught vanilla JavaScript. The frontend also used Bootstrap, an open-source CSS library, since its design functionality allows for easier mobile-responsiveness. For simple designs, each page will have a nav bar on top to link each of the pages together. One of the main aspects is the display of weekly carbon footprint. The display could be completed with a line graph, pie chart, or histogram. It was decided to use a histogram to give a clean and simple comparison as well as show a separation between the days. This histogram was created using the Chart JS open-source library.\u003C/p>\n\u003Ch2 id=\"final-result\">Final Result\u003C/h2>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/cftrackerInput.png&#x22;,&#x22;alt&#x22;:&#x22;Input Page&#x22;,&#x22;index&#x22;:0}\">\u003Cem>Input Page\u003C/em>\r\n\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/cftrackerFriends.png&#x22;,&#x22;alt&#x22;:&#x22;Friends Page&#x22;,&#x22;index&#x22;:0}\">\u003Cem>Friends Page\u003C/em>\r\n\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/cftrackerShow.png&#x22;,&#x22;alt&#x22;:&#x22;Race Page&#x22;,&#x22;index&#x22;:0}\">\u003Cem>Race Page\u003C/em>\r\n\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/cftrackerRaces.png&#x22;,&#x22;alt&#x22;:&#x22;Racelist Page&#x22;,&#x22;index&#x22;:0}\">\u003Cem>Race list Page\u003C/em>\r\n\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../images/coverCftracker.png&#x22;,&#x22;alt&#x22;:&#x22;Profile Page&#x22;,&#x22;index&#x22;:0}\">\u003Cem>Profiile page\u003C/em>\u003C/p>",{"headings":304,"localImagePaths":323,"remoteImagePaths":324,"frontmatter":325,"imagePaths":328},[305,308,309,312,313,316,319,320],{"depth":40,"slug":306,"text":307},"overiew","Overiew",{"depth":130,"slug":124,"text":125},{"depth":40,"slug":310,"text":311},"solution-greenfoot","Solution: Greenfoot",{"depth":130,"slug":127,"text":128},{"depth":156,"slug":314,"text":315},"functional-requirements","Functional Requirements",{"depth":156,"slug":317,"text":318},"constraints-and-criteria","Constraints and Criteria",{"depth":130,"slug":137,"text":138},{"depth":130,"slug":321,"text":322},"final-result","Final Result",[293,294,295,296,297,298,299],[],{"project_id":40,"title":281,"description":282,"categories":326,"tags":327,"image":288,"date_string":289},[61],[285,286,287],[293,294,295,296,297,298,299]]